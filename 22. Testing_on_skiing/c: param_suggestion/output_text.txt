10 : -1.00024
[[   0.    0.    0.    0.    0.    0.    0.    0.    0.    1.    0.    0.
     0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
     0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
     0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
     0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
     0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
     0.    0.    0.    0.    0.    0.    0.    0.    0.    0.]
 [   0.    0.    0.   17.   97.   24.    2.    7.    2.    0.    0.    0.
     0.    0.    0.    0.    0.    0.    2.    0.    0.    0.    0.    0.
     0.    0.    1.    0.    1.    0.    1.    2.    3.    1.    0.    0.
     0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
     0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
     0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
     0.    0.    0.    0.    0.    0.    0.    0.    0.    0.]
 [   0.    0.   19.  370. 2462.  398.   40.   34.   23.    2.    0.    0.
     0.    0.    0.    0.   20.   17.    9.    3.    0.    0.    0.    0.
    15.    5.    1.    2.    1.    4.    1.    0.    0.    0.    1.    0.
     0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
     0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
     0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
     0.    0.    0.    0.    0.    0.    0.    0.    0.    0.]
 [   0. 3942. 1138.  312.  522.  120.   62.   22.   18.   11.    0.    0.
     0.   15.   29.   49.   82.   45.   25.   97.  107.   50.   84.   55.
    36.    6.    3.    1.   26.    9.    7.   10.    5.    3.    3.    2.
     3.    1.    1.    0.    1.    0.    1.    0.    1.    0.    1.    0.
     0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
     0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
     0.    0.    0.    0.    0.    0.    0.    0.    0.    0.]
 [   0.    0.  190.  663.  816. 1026. 1194. 1041. 1027. 1179. 1052.  968.
  1130.  446.  107.   77.   44.   49.   45.   12.   36.   18.    6.    8.
     4.    8.    8.    7.    9.   13.   16.    8.   18.   20.    7.    3.
     0.    1.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
     0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
     0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
     0.    0.    0.    0.    0.    0.    0.    0.    0.    0.]
 [   0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
     0.   57.   48.   14.    9.    6.    3.    6.   15.    4.    5.    4.
     1.    6.    1.   10.    8.    7.    0.    0.    1.    1.    0.    0.
     1.    1.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
     0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
     0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
     0.    0.    0.    0.    0.    0.    0.    0.    0.    0.]
 [   0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
     0.    0.    0.    0.    0.    0.    4.   10.    2.    0.    6.   26.
    11.    3.    1.    4.    1.    0.    1.    0.    1.    0.    0.    0.
     0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
     0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
     0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
     0.    0.    0.    0.    0.    0.    0.    0.    0.    0.]
 [   0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
     0.    0.    0.    0.    0.    0.    0.    2.    0.    0.    0.    0.
     0.    2.    0.    0.    2.    0.    0.    0.    0.    0.    0.    0.
     0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
     0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
     0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
     0.    0.    0.    0.    0.    0.    0.    0.    0.    0.]]
Checking the total Q and that Qe samples -0.1528 -1.3957799687862396
Checking the total Q and that Qe samples -0.0913 -0.369354672986269
Test reward:  -1.0032400000000008
LR:  0.00036
replay buffer size:  22033
training steps:  9
Average siam loss:  -0.8649495174176991
Average value from last batch of unclaimed novelty:  tensor(-0.1063, dtype=torch.float64)
Average value from last batch of predicted nov:  -0.0588935090300204
Printing a sample from n step novelty returns, should be around 0 on average, going up to 9 max:  [-1.29341779 -1.23366012 -1.82575653 -2.61724672 -2.58042534 -2.60616393
 -2.47118497 -2.36196863 -1.45003731 -1.28508503 -0.94807269 -0.59332286
 -0.21990198  0.24248385]
then printing the same exp r:  [-0.5478157997131348, 0.1326151043176651, 0.27541422843933105, -0.5691230297088623, -0.40464839339256287, -0.5667206048965454, -0.5338538289070129, -0.5667206048965454, -0.5338538289070129, -0.40464839339256287, -0.40464839339256287, -0.40464839339256287, -0.47395962476730347, 0.2552461624145508]
Printing a sample from n step novelty returns, should be around 0 on average, going up to 9 max:  [-1.05789755 -0.98574408 -1.59022396 -2.34388602 -2.60402904 -2.43164052
 -2.38745282 -2.24096275 -2.24503784 -2.41962608 -1.7210028  -1.22652385
 -0.88642935 -0.36636292  0.18107543  0.66456534]
then printing the same exp r:  [-0.5478157997131348, 0.1326151043176651, 0.27541422843933105, -0.1259973645210266, -0.5667206048965454, -0.5667206048965454, -0.5667206048965454, -0.5338538289070129, -0.3635551631450653, -0.35644716024398804, -0.5338538289070129, -0.40464839339256287, -0.5667206048965454, -0.5667206048965454, -0.47395962476730347, 0.6995424628257751]
20 : -1.00072
[[   0.    0.    0.    0.    0.    3.    0.    0.    3.    3.    0.    0.
     0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
     0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
     0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
     0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
     0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
     0.    0.    0.    0.    0.    0.    0.    0.    0.    0.]
 [   0.    0.    0.   37.  371.   48.    2.   38.    9.    1.    0.    0.
     0.    0.    0.    0.    0.    0.    3.    0.    0.    0.    0.    0.
     0.    0.    1.    0.    1.    0.    1.    2.    3.    1.    0.    0.
     0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
     0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
     0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
     0.    0.    0.    0.    0.    0.    0.    0.    0.    0.]
 [   0.    0.   42.  777. 4411.  772.  119.   66.   33.    3.    1.    0.
     0.    0.    0.    0.   48.   21.   10.    4.    0.    0.    0.    0.
    15.    5.    1.    2.    1.    4.    1.    0.    0.    0.    1.    0.
     0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
     0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
     0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
     0.    0.    0.    0.    0.    0.    0.    0.    0.    0.]
 [   0. 7659. 2141.  484. 1023.  201.  127.   29.   20.   12.    0.    0.
     0.   53.   49.   88.   90.   51.   30.  116.  122.   51.   84.   55.
    36.    6.    3.    1.   26.    9.    7.   10.    5.    3.    3.    2.
     3.    1.    1.    0.    1.    0.    1.    0.    1.    0.    1.    0.
     0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
     0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
     0.    0.    0.    0.    0.    0.    0.    0.    0.    0.]
 [   0.    0.  332. 1408. 1854. 2447. 2657. 2080. 2304. 2681. 2001. 1945.
  1982.  950.  170.  151.   54.   56.   51.   19.   36.   18.    6.    8.
     4.    8.    8.    7.    9.   13.   16.    8.   18.   20.    7.    3.
     0.    1.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
     0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
     0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
     0.    0.    0.    0.    0.    0.    0.    0.    0.    0.]
 [   0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
     0.  109.   71.   21.   11.   10.    4.   15.   21.    4.    8.    4.
     1.    6.    1.   10.    8.    7.    0.    0.    1.    1.    0.    0.
     1.    1.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
     0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
     0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
     0.    0.    0.    0.    0.    0.    0.    0.    0.    0.]
 [   0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
     0.    0.    0.    0.    0.    0.    4.   10.    5.    2.   31.   57.
    13.    3.    1.    4.    1.    0.    1.    0.    1.    0.    0.    0.
     0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
     0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
     0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
     0.    0.    0.    0.    0.    0.    0.    0.    0.    0.]
 [   0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
     0.    0.    0.    0.    0.    0.    0.    2.    0.    2.    0.    0.
     0.    2.    0.    0.    2.    0.    0.    0.    0.    0.    0.    0.
     0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
     0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
     0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.
     0.    0.    0.    0.    0.    0.    0.    0.    0.    0.]]
Test reward:  -1.0032400000000006
LR:  0.0007599999999999999
replay buffer size:  43257
training steps:  19
Average siam loss:  -0.9087356915697455
Average value from last batch of unclaimed novelty:  tensor(-0.5735, dtype=torch.float64)
Average value from last batch of predicted nov:  -0.16703329085951646
Printing a sample from n step novelty returns, should be around 0 on average, going up to 9 max:  [-1.4929516  -1.0226552  -0.24059184  0.33547031  0.50433441  1.39974318
  1.87001743  1.12080068  0.54850305  0.50390362  1.36117039  0.63111584
 -0.08053466 -0.06414895 -0.26988161 -0.46645297 -0.96008753 -0.68386957
 -0.55779845  0.29493324  0.43261567  0.65516147  0.69347227  1.17650697
  1.26743801  0.95636742  0.99116295  0.56599867  0.08425039  0.12737424
  0.17483553  0.8574862   0.9954222   1.0989501   1.0752058   0.96563467
  0.48145722]
then printing the same exp r:  [-0.5118476748466492, -0.42248597741127014, -0.42248597741127014, -0.3999500274658203, -0.16631406545639038, 0.10396014899015427, 0.840203583240509, 0.5618450045585632, 0.33292606472969055, -0.09493362158536911, 0.7293062210083008, 0.5183648467063904, -0.13240180909633636, -0.02414587140083313, 0.6451463103294373, 0.4822899401187897, -0.3348793387413025, -0.18661406636238098, -0.3348793387413025, -0.022882618010044098, 0.40337151288986206, -0.04314954951405525, -0.06436918675899506, -0.0211520716547966, 0.43467485904693604, -0.030260415747761726, 0.43153074383735657, 0.4657376706600189, 0.018207382410764694, 0.08158378303050995, -0.05113726481795311, -0.05113726481795311, -0.05113726481795311, 0.08158378303050995, 0.16616091132164001, 0.5350003242492676, 0.5067970752716064]
Printing a sample from n step novelty returns, should be around 0 on average, going up to 9 max:  [-0.82010588 -0.3648649   0.14224105  0.55986681  0.29861328]
then printing the same exp r:  [-0.49840444326400757, -0.5263093709945679, -0.41013938188552856, 0.2907201945781708, 0.31432977318763733]
Checking the total Q and that Qe samples -0.1492 -2.2670024144276977
Checking the total Q and that Qe samples 0.8992537104203756 0.9776865900687568
Printing a sample from n step novelty returns, should be around 0 on average, going up to 9 max:  [-1.00842494 -1.48131484 -1.3987077  -1.45714641 -1.37659653 -1.44164145
 -2.30617504 -2.41076255 -2.33739297 -2.38806191 -2.41536312 -2.4223353
 -2.41155436 -2.54622614 -1.92217879 -2.29935048 -2.31044423 -1.35019694
 -0.74695942 -0.92907255 -0.65720785 -0.61260183 -0.64681981 -1.28866141
 -1.94720946 -1.82006833 -2.42118025 -2.41296722 -2.3777272  -2.47994936
 -2.66507258 -2.67989393 -2.70531489 -2.40784898 -2.49171719 -2.34924189
 -2.2580281  -2.10325337 -1.95400766 -1.51269606 -1.42136461 -1.2734765
 -1.21857903 -1.18083122 -0.8965103  -0.97741744 -0.58075673  0.11005683
  0.56495811  0.63909202  0.71241313  0.87106498  0.70924229  0.32922602
  0.09468862]
then printing the same exp r:  [-0.37392309308052063, -0.36748260259628296, -0.1935063749551773, -0.33557069301605225, -0.049306560307741165, 0.6723083853721619, -0.19511902332305908, -0.3166011571884155, -0.3166011571884155, -0.14028528332710266, -0.236501544713974, -0.3166011571884155, -0.236501544713974, -0.401793897151947, -0.14028528332710266, -0.236501544713974, -0.3166011571884155, -0.401793897151947, -0.16691537201404572, -0.07814683765172958, -0.24847175180912018, -0.2559164762496948, 0.31383857131004333, 0.40456464886665344, -0.35580071806907654, 0.22740912437438965, -0.3935358226299286, -0.4401480257511139, -0.3008311092853546, -0.22331015765666962, -0.4194055199623108, -0.3935358226299286, -0.4194055199623108, -0.4194055199623108, -0.4194055199623108, -0.4401480257511139, -0.4194055199623108, -0.3902246654033661, 0.09568075835704803, -0.2627299427986145, -0.36547571420669556, -0.2627299427986145, -0.24269093573093414, -0.30762699246406555, -0.0654371902346611, 0.22538411617279053, -0.22701530158519745, -0.22701530158519745, 0.005156783852726221, -0.17319153249263763, -0.12115642428398132, 0.2076682150363922, 0.4173448085784912, 0.25186508893966675, 0.09967222809791565]
Printing a sample from n step novelty returns, should be around 0 on average, going up to 9 max:  [-3.26152298 -3.21564218 -3.19316182 -3.157825   -3.20964104 -3.16474207
 -3.13151264 -3.12878298 -3.20443048 -3.05842067 -3.10358585 -3.12852474
 -2.03140775 -2.14528896 -2.52562312 -2.44772134 -2.19890443 -1.96013024
 -2.51863218 -2.72946973 -2.56507466 -2.71222042 -2.77273938 -2.62774941
 -2.52992703 -2.56515106 -2.18619002 -2.47869259 -2.62369341 -2.70550653
 -2.9280369  -2.35417073 -3.07182365 -3.06026997 -2.55748527 -2.54228487
 -2.52750719 -2.55586849 -2.42946545 -2.40091924 -2.69582811 -2.70850225
 -2.68729227 -2.71278753 -2.68899728 -2.74105498 -2.75624568 -2.77176066
 -2.86335722 -2.796137   -2.87714756 -2.81065314 -2.92663985 -2.9291188
 -2.93172823 -2.95695437 -2.87523311 -2.77086612 -2.28515182 -2.4879042
 -2.48754346 -2.53294291 -2.56712239 -2.56175532 -2.69000342 -2.53403606
 -2.6288817  -2.63960065 -2.78235165 -2.30711592 -2.49779242 -2.52938825
 -2.63117379 -1.76750518 -1.18737711 -1.07536149 -1.45489472 -1.22550207
 -1.05834872 -1.36126185 -0.95095768 -1.42417097 -1.52853697 -1.55499873
 -1.64726363 -1.62049093 -1.70671431 -1.70295209 -1.62873714 -1.61281423
 -1.61752355 -1.64828914 -1.50729309 -1.53780282 -1.51709389 -1.45137598
 -1.23888878 -0.7795232  -0.4632913  -0.17504664 -0.04600991 -0.42387407
 -0.45182166 -0.47542434 -0.37456386 -0.14691784]
then printing the same exp r:  [-0.5738832950592041, -0.5293591618537903, -0.49475112557411194, -0.5051805973052979, -0.5051805973052979, -0.5293591618537903, -0.5051805973052979, -0.4453686773777008, -0.5738832950592041, -0.4453686773777008, -0.49475112557411194, -0.5051805973052979, -0.5293591618537903, -0.40384024381637573, -0.49475112557411194, -0.5293591618537903, -0.5193464159965515, 0.17141537368297577, -0.20564384758472443, -0.44263842701911926, -0.27177998423576355, -0.28874292969703674, -0.44263842701911926, -0.44263842701911926, -0.24964483082294464, -0.44263842701911926, -0.3425993025302887, -0.27177998423576355, -0.3425993025302887, -0.27177998423576355, -0.4293260872364044, -0.01262289471924305, -0.45973673462867737, -0.45973673462867737, -0.5445048809051514, -0.4222468137741089, -0.3295377492904663, -0.44879138469696045, -0.481432169675827, -0.01932425983250141, -0.4252193868160248, -0.36216238141059875, -0.31434082984924316, -0.49158790707588196, -0.31434082984924316, -0.45409470796585083, -0.32795023918151855, -0.32795023918151855, -0.49158790707588196, -0.36216238141059875, -0.49158790707588196, -0.32795023918151855, -0.4252193868160248, -0.4252193868160248, -0.45409470796585083, -0.4252193868160248, -0.45409470796585083, -0.4252193868160248, -0.4252193868160248, -0.49158790707588196, -0.31434082984924316, -0.32795023918151855, -0.36929571628570557, -0.3387717604637146, -0.5263634324073792, -0.35646018385887146, -0.35646018385887146, -0.35646018385887146, -0.49005573987960815, -0.35646018385887146, -0.47163304686546326, -0.35646018385887146, -0.5263634324073792, -0.4217481315135956, -0.2331627607345581, 0.028440779075026512, -0.5008548498153687, -0.5123969912528992, 0.016306594014167786, -0.12720657885074615, -0.02755468524992466, -0.2794603407382965, -0.39041516184806824, -0.3260047435760498, -0.4223407506942749, -0.173776775598526, -0.4024592638015747, -0.4380735158920288, -0.4380735158920288, -0.4024592638015747, -0.22907838225364685, -0.4024592638015747, -0.3576914668083191, -0.4380735158920288, -0.4197944700717926, -0.22907838225364685, -0.22907838225364685, -0.4024592638015747, -0.4380735158920288, -0.3576914668083191, -0.30501046776771545, 0.00563842011615634, -0.00017740104522090405, -0.1258828192949295, -0.24735990166664124, -0.15465036034584045]
30 : -1.0005733333333333
[[    0.     0.     0.     0.     0.     6.     0.     0.     9.     4.
      1.     1.     0.     1.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.]
 [    0.     0.     0.    61.   560.    68.     2.   103.    23.     4.
      5.     2.     1.     0.     0.     0.     0.     6.     6.     0.
      0.     0.     0.     0.     0.     0.     1.     0.     1.     0.
      1.     2.     3.     1.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.]
 [    0.     0.    66.  1151.  5228.  1002.   222.   156.    59.     7.
      2.     2.     0.     0.     0.     0.    76.    34.    21.     4.
      0.     0.     0.     0.    15.     5.     2.     3.    15.     4.
      1.     0.     0.     0.     1.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.]
 [    0. 11759.  3234.   704.  1445.   322.   218.    60.    35.    18.
      0.     0.     0.   116.    92.   195.   248.    92.    65.   160.
    154.    96.   134.    63.    37.     7.     4.     1.    26.     9.
      7.    10.     5.     3.     3.     2.     3.     1.     1.     0.
      1.     0.     1.     0.     1.     0.     1.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.]
 [    0.     0.   647.  2776.  3331.  4584.  4503.  3917.  4049.  5375.
   3532.  3219.  2992.  1555.   260.   350.   130.   131.    98.   151.
    118.    86.    11.     9.     7.     9.    10.    11.    15.    19.
     20.     8.    18.    20.     7.     3.     0.     1.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.]
 [    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.   249.   115.    31.    14.    16.    19.    40.
     30.     9.    21.     5.     1.    22.     5.    11.     8.     7.
      0.     0.     1.     1.     0.     0.     1.     1.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.]
 [    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     4.    49.
     20.    50.   107.    70.    23.     6.     7.     6.     1.     0.
      1.     0.     1.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.]
 [    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     2.
      1.     2.     1.     0.     0.     4.     0.     1.     3.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.]]
Checking the total Q and that Qe samples -0.1256409 -0.6103905434918403
Checking the total Q and that Qe samples -0.14915677462509125 -2.608469225377009
Test reward:  -1.0028000000000006
LR:  0.001
replay buffer size:  71219
training steps:  29
Average siam loss:  -0.9214977761730552
Average value from last batch of unclaimed novelty:  tensor(-0.8332, dtype=torch.float64)
Average value from last batch of predicted nov:  -0.26088097170685076
Checking the total Q and that Qe samples 0.4663431875231147 0.6741335891534239
Checking the total Q and that Qe samples 2.4451665326654637 2.5661587693774073
Checking the total Q and that Qe samples -0.1866 -2.831784349130213
Printing a sample from n step novelty returns, should be around 0 on average, going up to 9 max:  [1.38550254 1.71065801 1.96883209 1.2923874  0.65763859 0.5210842
 0.5210842  0.5210842  0.59286328 0.59664112 1.12760994 1.02604531
 1.47974978 1.7270635  1.947238   1.74927621 1.40228501 1.28521253
 0.58251634]
then printing the same exp r:  [-0.13997291028499603, -0.055878087878227234, 0.8923287987709045, 0.815030574798584, 0.2834283113479614, 0.13968685269355774, 0.13968685269355774, 0.13968685269355774, 0.13968685269355774, 0.13968685269355774, 0.13968685269355774, 0.13968685269355774, 0.232450932264328, 0.13968685269355774, 0.3004480004310608, 0.4390583634376526, 0.19087696075439453, 0.7703389525413513, 0.613175094127655]
Printing a sample from n step novelty returns, should be around 0 on average, going up to 9 max:  [-2.12797371 -3.11539389 -2.83334721 -2.64879559 -2.4164595  -2.21712189
 -2.37946134 -2.8997014  -2.87345773 -2.89894678 -2.92577736 -2.97120635
 -2.97050493 -2.82019556 -2.68400002 -2.73238199 -2.78197645 -2.86641826
 -2.74274472 -3.29315488 -3.30361149 -2.73544612 -2.48324964 -2.72931944
 -2.82159377 -2.6819117  -2.86723756 -3.55782715 -3.58328019 -3.49347117
 -3.69637799 -3.65344961 -2.90840647 -2.35142199 -2.07482457 -1.49300854
 -0.97463483 -1.18053912 -2.01839994 -1.74157778 -1.36147861 -2.17775869
 -2.14192468 -2.23408044 -2.43487494 -2.77660472 -3.56970501 -3.58749833
 -3.56277904 -3.48229017 -3.6416231  -3.58977909 -3.65027515 -3.59888652
 -3.61735011 -3.45641935 -3.53864302 -3.44482812 -3.52644172 -3.67569535
 -3.76666303 -3.68947176 -3.61565093 -3.59241336 -3.56756987 -3.48733315
 -3.56183619 -3.57834977 -3.57764417 -3.48588196 -3.48880069 -3.50233588
 -3.56522548 -3.64892442 -3.57513365 -3.59235607 -3.58684637 -3.60554186
 -3.43439234 -3.57880126 -3.65811842 -3.63711598 -2.14931308 -1.83338768
 -1.31086226 -0.87080887 -0.40759478]
then printing the same exp r:  [-0.4801124930381775, -0.4801124930381775, -0.4911964535713196, -0.4881691038608551, -0.44294241070747375, -0.0707738995552063, 0.27283889055252075, -0.3191945552825928, -0.2660805881023407, -0.2660805881023407, -0.2660805881023407, -0.2735034227371216, -0.2969837784767151, -0.2969837784767151, -0.2969837784767151, -0.3191945552825928, -0.2660805881023407, -0.2955266237258911, 0.23153576254844666, -0.38784676790237427, -0.4148270785808563, -0.38784676790237427, -0.15119889378547668, -0.34961479902267456, -0.4148270785808563, -0.22239872813224792, 0.2731110155582428, -0.46004176139831543, -0.5009698867797852, -0.27920082211494446, -0.46004176139831543, -0.46004176139831543, -0.5009698867797852, -0.40317273139953613, -0.5009698867797852, -0.40317273139953613, -0.13251084089279175, 0.5744378566741943, -0.1061553955078125, -0.2978069484233856, 0.5528225898742676, -0.4704592227935791, -0.35953736305236816, -0.2557489275932312, -0.04452616348862648, 0.3269568681716919, -0.5090461373329163, -0.5335413217544556, -0.5335413217544556, -0.4290471374988556, -0.5090461373329163, -0.5335413217544556, -0.5090461373329163, -0.4386533796787262, -0.6190195083618164, -0.4386533796787262, -0.6190195083618164, -0.4386533796787262, -0.44145020842552185, -0.44145020842552185, -0.5335413217544556, -0.5335413217544556, -0.5335413217544556, -0.6190195083618164, -0.5335413217544556, -0.4290471374988556, -0.4386533796787262, -0.5090461373329163, -0.6190195083618164, -0.4386533796787262, -0.5090461373329163, -0.44145020842552185, -0.5090461373329163, -0.5335413217544556, -0.4290471374988556, -0.5335413217544556, -0.5090461373329163, -0.6190195083618164, -0.44145020842552185, -0.4290471374988556, -0.5335413217544556, -0.5335413217544556, -0.4290471374988556, -0.6190195083618164, -0.5090461373329163, -0.5090461373329163, -0.4290471374988556]
Checking the total Q and that Qe samples -0.2056 -3.0474397486216054
Printing a sample from n step novelty returns, should be around 0 on average, going up to 9 max:  [-0.66668088 -0.79369311 -0.71318689 -0.60788111 -0.665149   -0.65770927
 -1.01950832 -1.47469597 -1.25022756 -0.90661983 -1.43381011 -1.33199867
 -0.21001072  0.52115775  1.32233192  2.29278591  2.68086908  2.45826072
  2.95339375  3.90147939  4.0280392   3.47834496  3.71620984  3.3765556
  2.70844294  2.40811313  2.46202896  2.03500699  2.12392491  2.15558573
  2.12660552  2.76749771  2.95519627  2.66949296  2.71023685  2.59238779
  2.22201101  1.74420879  1.6247535   1.60833818  1.44283598  1.09936319
  0.79093718  0.76953263  0.64223729  0.6961912   0.90307331  1.2026185
  1.16398555  1.39548842  1.89643333  2.30451589  2.15264213  2.73870683
  2.66536939  2.57600322  2.34072888  2.81582855  2.39510454  2.6838061
  2.66816768  2.76386976  3.10671887  3.06515892  2.71643979  2.82984482
  2.5451257   3.26797888  2.83950987  3.16844212  3.57376877  3.95187557
  3.97494764  3.76713473  3.92170177  4.02039812  3.99966343  3.86749716
  4.0137925   4.16778759  4.05985987  3.97449194  4.60091888  4.49539019
  4.27172972  3.7899858   3.49190433  3.3020793   2.77220789  2.73078716
  2.94948152  3.23292137  3.45736225  3.79148453  3.77530633  3.81510743
  3.42795583  3.05334822  3.69807005  3.55555307  3.54042713  2.38101161
  1.64061203  1.05385467  0.72042651  0.36944949]
then printing the same exp r:  [-0.4801124930381775, -0.4881691038608551, -0.44294241070747375, 0.5025623440742493, 0.5689218640327454, 0.5233153104782104, 0.04991958290338516, -0.5033733248710632, -0.13251084089279175, 0.6534276604652405, 0.5558624267578125, -0.5538047552108765, -0.35953736305236816, -0.2557489275932312, -0.04452616348862648, 0.042305879294872284, 0.3522059917449951, 0.1118272915482521, 0.33457690477371216, 0.4037635028362274, 0.7591527700424194, 0.4342264235019684, 0.47278496623039246, 0.8577383160591125, 0.6679136753082275, 0.08473917096853256, 0.5685146450996399, 0.3830804228782654, 0.0920369029045105, 0.3674663305282593, 0.0920369029045105, 0.0920369029045105, 0.5482131838798523, 0.0920369029045105, 0.3674663305282593, 0.6548980474472046, 0.7017233967781067, 0.20353691279888153, 0.05531398579478264, 0.20353691279888153, 0.25666120648384094, 0.20353691279888153, 0.05531398579478264, 0.005046747159212828, 0.005046747159212828, -0.14504177868366241, -0.14504177868366241, 0.05531398579478264, -0.14504177868366241, 0.09785514324903488, 0.09785514324903488, 0.20353691279888153, 0.005046747159212828, 0.09785514324903488, 0.49553266167640686, 0.6367373466491699, 0.1405327469110489, 0.715321958065033, -0.08762402832508087, 0.41059017181396484, 0.41059017181396484, 0.6947762370109558, 0.5073225498199463, 0.15118330717086792, 0.3821006119251251, 0.5270557999610901, 0.5375174880027771, 0.698394775390625, 0.20511482656002045, 0.26199015974998474, 0.1802501082420349, 0.6854780912399292, 0.9175773859024048, 0.5442599058151245, 0.7149603366851807, 0.5465756058692932, 0.7149603366851807, 0.7149603366851807, 0.7149603366851807, 0.9438477158546448, 0.8890847563743591, 0.5223093628883362, 0.8911705017089844, 0.8911705017089844, 0.8380024433135986, 0.8037256002426147, 0.8045200109481812, 0.8350957036018372, 0.6182369589805603, 0.23120246827602386, 0.4570293724536896, 0.6182369589805603, 0.23120246827602386, 0.6182369589805603, 0.4570293724536896, 0.8860768675804138, 0.9276875257492065, 0.5531404614448547, 0.5778872966766357, 0.4429953694343567, 0.4429953694343567, 0.8657159805297852, 0.6731053590774536, 0.3888942003250122, 0.3888942003250122, 0.3888942003250122]
40 : -1.0006000000000002
[[    0.     0.     0.     0.     2.     7.     0.     2.    14.    12.
      3.     1.     0.     3.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.]
 [    0.     0.     0.    86.   664.    85.     4.   206.   146.    11.
     11.     4.     5.     1.     1.     0.     0.    45.    16.     0.
      0.     0.     0.     0.     0.     0.     1.     1.     1.     0.
      1.     2.     3.     1.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.]
 [    0.     0.    92.  1441.  5445.  1097.   572.   247.    85.    14.
      5.     3.     1.     0.    50.     1.   267.    59.    39.     6.
      0.     0.     0.     0.    16.    11.     8.     4.    28.     4.
      1.     0.     0.     0.     1.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.]
 [    0. 17911.  4622.   894.  1647.   462.   291.    86.    45.    24.
      0.     0.     0.   149.   141.   344.   659.   164.   167.   331.
    377.   224.   279.   172.    57.    45.    43.    36.    56.    31.
     29.    10.     5.     3.     3.     2.     3.     1.     1.     0.
      1.     0.     1.     0.     1.     0.     1.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.]
 [    0.     0.  1421.  5222.  6084.  7594.  7355.  6501.  6372.  7468.
   4839.  3888.  3740.  2266.   443.   958.   325.   340.   193.   204.
    280.   132.    41.    16.    10.   111.    77.    15.    62.    53.
     68.    11.    24.    27.     8.     3.     0.     1.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.]
 [    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.   442.   207.    46.    29.    23.   659.   374.
     44.    71.    52.   215.     2.   174.    12.    42.    13.     8.
     26.     1.     2.     1.     0.     0.     1.     1.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.]
 [    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.    68.   200.
     28.    74.   231.   187.   157.    33.    23.    33.     3.    15.
      2.     4.     1.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.]
 [    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     4.
      2.     3.     1.     0.     0.    14.     1.     2.     3.     2.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.]]
Printing a sample from n step novelty returns, should be around 0 on average, going up to 9 max:  [-3.56386515 -3.5470917  -3.5330444  -3.44828495 -3.44263834 -3.45717789
 -3.48534015 -3.46784962 -3.53648907 -3.52616514 -3.51365954 -3.52852122
 -3.62885284 -3.56009388 -3.56253075 -3.57869693 -3.54690424 -3.46287426
 -3.44603275 -3.44852534 -3.43357173 -3.45784846 -3.46850504 -3.48313832
 -3.484168   -3.46319996 -3.44454416 -3.44160663 -3.43724683 -3.44681911
 -3.48539412 -3.52599939 -3.49143315 -3.47873784 -3.47759999 -3.46417662
 -3.44740366 -3.47505149 -3.47565556 -3.43843964 -3.4480747  -3.4616327
 -3.46153051 -3.47364856 -3.51363564 -3.50210889 -3.48733237 -3.49954074
 -3.49785948 -3.49646034 -3.50914913 -3.58337806 -3.6439364  -3.61557737
 -3.62037392 -3.6266906  -3.52480797 -3.43697192 -3.44201879 -3.42672274
 -3.47739777 -3.50398113 -3.48747171 -3.51722838 -3.51740918 -3.43182682
 -3.43092863 -3.45770935 -3.43534875 -3.43727901 -3.41384313 -3.41199842
 -3.4066408  -3.43214322 -3.44144689 -3.47973915 -3.48136265 -3.49667264
 -3.49841466 -3.55450124 -3.55608325 -3.57532591 -3.54499409 -3.56150497
 -3.45619152 -3.4389983  -3.43450121 -3.46146996 -3.44555935 -3.47046701
 -3.47424561 -3.4606457  -3.43843964 -3.43711676 -3.43914008 -3.41580215
 -3.41328782 -3.43914008 -3.43785407 -3.41444845 -3.44036179 -3.41708816
 -3.3925896  -3.3998115  -3.40290256 -3.41066725 -3.44860721 -3.47631839
 -3.44077565 -3.46807464 -3.45414994 -3.45007969 -3.46032739 -3.49961335
 -3.50151015 -3.50086367 -3.48702641 -3.4863248  -3.45130872 -3.46288878
 -3.46626852 -3.4443584  -3.42754718 -3.43209802 -3.3925896  -3.3998115
 -3.43591243 -3.43445654 -3.52983738 -3.57278176 -3.54231605 -3.53229879
 -3.51386395 -3.428095   -3.42302405 -3.44938874 -3.46086775 -3.4671723
 -3.44530975 -3.44114484 -3.41791242 -3.39970931 -3.49326135 -3.53428069
 -3.52119742 -3.55008603 -3.62305372 -3.52097968 -3.48778753 -3.48983464
 -3.49463258 -3.40785145 -3.44437553 -3.44981206 -3.44986046 -3.44649558
 -3.44559669 -3.43048895 -3.52566097 -3.54457013 -3.55224889 -3.56787391
 -3.58558899 -3.50359504 -3.46784962 -3.44602272 -3.40888547 -3.4011652
 -3.40883845 -3.43445654 -3.47364856 -3.5026777  -3.50153216 -3.48936838
 -3.50295158 -3.48658112 -3.48514891 -3.48434848 -3.49766747 -3.49461994
 -3.49966975 -3.48293341 -3.53656689 -3.50255684 -2.17695933 -2.95476134
 -2.96315786 -2.75789615 -2.62480039 -2.52328305 -2.8965666  -2.8404782
 -2.83241792 -2.81863159 -2.60821991 -2.45384458 -2.53340769 -2.55095415
 -2.3302267  -2.5216217  -2.88879673 -2.45966756 -2.16695342 -2.33655422
 -2.71840863 -0.92131151 -0.59708492 -0.9527041  -0.56849122 -0.27050988]
then printing the same exp r:  [-0.44959136843681335, -0.46539121866226196, -0.5380071401596069, -0.46539121866226196, -0.44617554545402527, -0.4210924506187439, -0.44617554545402527, -0.44959136843681335, -0.4637529253959656, -0.46539121866226196, -0.44959136843681335, -0.4210924506187439, -0.5380071401596069, -0.44959136843681335, -0.44959136843681335, -0.46539121866226196, -0.5380071401596069, -0.4637529253959656, -0.44617554545402527, -0.4637529253959656, -0.4210924506187439, -0.44959136843681335, -0.44617554545402527, -0.44959136843681335, -0.44959136843681335, -0.44617554545402527, -0.4637529253959656, -0.4637529253959656, -0.44959136843681335, -0.4210924506187439, -0.4210924506187439, -0.46539121866226196, -0.4637529253959656, -0.4637529253959656, -0.4637529253959656, -0.4637529253959656, -0.4210924506187439, -0.44959136843681335, -0.46539121866226196, -0.44959136843681335, -0.44617554545402527, -0.44959136843681335, -0.44959136843681335, -0.4210924506187439, -0.4637529253959656, -0.4637529253959656, -0.44959136843681335, -0.46539121866226196, -0.4637529253959656, -0.44959136843681335, -0.44617554545402527, -0.4637529253959656, -0.46539121866226196, -0.4637529253959656, -0.4637529253959656, -0.5380071401596069, -0.5380071401596069, -0.4210924506187439, -0.4637529253959656, -0.46539121866226196, -0.4210924506187439, -0.44617554545402527, -0.4210924506187439, -0.44959136843681335, -0.5380071401596069, -0.44617554545402527, -0.4210924506187439, -0.44959136843681335, -0.44617554545402527, -0.44959136843681335, -0.44617554545402527, -0.44959136843681335, -0.4210924506187439, -0.44959136843681335, -0.4210924506187439, -0.44617554545402527, -0.44617554545402527, -0.44959136843681335, -0.4637529253959656, -0.4637529253959656, -0.44617554545402527, -0.4637529253959656, -0.44959136843681335, -0.5380071401596069, -0.4637529253959656, -0.4637529253959656, -0.4210924506187439, -0.46539121866226196, -0.4210924506187439, -0.44617554545402527, -0.4637529253959656, -0.44959136843681335, -0.44959136843681335, -0.44617554545402527, -0.44959136843681335, -0.44959136843681335, -0.4210924506187439, -0.44959136843681335, -0.44959136843681335, -0.4210924506187439, -0.44959136843681335, -0.44959136843681335, -0.44959136843681335, -0.4210924506187439, -0.44959136843681335, -0.4210924506187439, -0.4210924506187439, -0.4637529253959656, -0.4210924506187439, -0.4637529253959656, -0.46539121866226196, -0.44959136843681335, -0.4210924506187439, -0.44959136843681335, -0.44959136843681335, -0.46539121866226196, -0.4637529253959656, -0.4637529253959656, -0.44959136843681335, -0.44617554545402527, -0.44959136843681335, -0.46539121866226196, -0.4210924506187439, -0.46539121866226196, -0.44959136843681335, -0.4210924506187439, -0.44959136843681335, -0.4210924506187439, -0.4210924506187439, -0.4637529253959656, -0.4637529253959656, -0.44959136843681335, -0.5380071401596069, -0.4637529253959656, -0.4210924506187439, -0.44959136843681335, -0.4210924506187439, -0.44959136843681335, -0.4637529253959656, -0.44959136843681335, -0.46539121866226196, -0.4210924506187439, -0.4210924506187439, -0.4637529253959656, -0.4210924506187439, -0.44959136843681335, -0.5380071401596069, -0.4637529253959656, -0.44617554545402527, -0.44617554545402527, -0.5380071401596069, -0.4210924506187439, -0.4210924506187439, -0.44617554545402527, -0.44959136843681335, -0.44959136843681335, -0.4637529253959656, -0.4210924506187439, -0.44617554545402527, -0.44617554545402527, -0.44959136843681335, -0.44959136843681335, -0.5380071401596069, -0.46539121866226196, -0.44959136843681335, -0.4637529253959656, -0.46539121866226196, -0.44959136843681335, -0.4210924506187439, -0.4210924506187439, -0.4210924506187439, -0.4637529253959656, -0.4637529253959656, -0.44959136843681335, -0.46539121866226196, -0.44959136843681335, -0.4637529253959656, -0.44959136843681335, -0.46539121866226196, -0.44617554545402527, -0.44617554545402527, -0.46539121866226196, -0.46539121866226196, -0.4637529253959656, -0.44959136843681335, -0.4210924506187439, -0.5380071401596069, -0.4210924506187439, -0.39059844613075256, 0.09763491898775101, -0.33615756034851074, -0.3343488872051239, -0.29047924280166626, -0.30093368887901306, -0.29047924280166626, -0.2708018720149994, -0.3343488872051239, -0.2708018720149994, -0.0023406094405800104, 0.14922364056110382, -0.40670454502105713, -0.40327054262161255, -0.18441712856292725, 0.10128163546323776, -0.23794269561767578, -0.37271666526794434, 0.32419365644454956, -0.4343551993370056, -0.3279019296169281, -0.28474724292755127]
Checking the total Q and that Qe samples -0.23258736605316163 -3.328946636236408
Test reward:  -1.0022800000000005
LR:  0.001
replay buffer size:  75000
training steps:  39
Average siam loss:  -0.9464203976094723
Average value from last batch of unclaimed novelty:  tensor(-1.2069, dtype=torch.float64)
Average value from last batch of predicted nov:  -0.39192857850064605
Printing a sample from n step novelty returns, should be around 0 on average, going up to 9 max:  [3.03953    2.93237427 2.49442904 2.13270221 1.82757489 1.7294705
 1.72519666 1.74346645 1.73992923 1.73620585 1.7294705  1.72519666
 1.74346645 1.77979539 2.05205    2.32618573 2.27543816 2.40054783
 2.38970468 3.08632182 2.89002968 2.59967635 2.1933929  1.75327804
 0.93022608]
then printing the same exp r:  [0.7149409055709839, 0.9622108340263367, 0.9408224821090698, 0.8651847243309021, 0.6517345309257507, 0.5431085228919983, 0.44246014952659607, 0.5431085228919983, 0.5431085228919983, 0.55555659532547, 0.5431085228919983, 0.44246014952659607, 0.5431085228919983, 0.5431085228919983, 0.55555659532547, 0.5431085228919983, 0.44246014952659607, 0.5946297645568848, 0.8970585465431213, 0.8970585465431213, 0.44246014952659607, 0.5431085228919983, 0.55555659532547, 0.9153297543525696, 0.9791853427886963]
Checking the total Q and that Qe samples 6.533949046361209 6.731649355387862
Printing a sample from n step novelty returns, should be around 0 on average, going up to 9 max:  [-3.07001218 -1.14232378 -0.30562607 -0.29395943 -0.14911568 -0.14980844
  0.48007647  0.33342287 -0.02954413  0.0960942   0.31616886  0.49744127]
then printing the same exp r:  [-0.376952201128006, -0.300216943025589, -0.300216943025589, -0.32686033844947815, -0.16077637672424316, -0.17253202199935913, 0.22180217504501343, 0.38051557540893555, -0.12719328701496124, -0.21501706540584564, -0.1646319478750229, 0.5236223936080933]
50 : -1.0001733333333334
[[    0.     0.     0.     0.     2.     7.     0.     3.    23.    21.
      4.     1.     0.     3.     0.     0.     0.     0.     1.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.]
 [    0.     0.     0.   111.   713.   104.     6.   296.   186.    17.
     16.     5.     7.     2.     3.     1.     0.   150.    20.     0.
      0.     0.     0.     0.     0.     0.     1.     1.     1.     0.
      1.     2.     3.     1.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.]
 [    0.     0.   110.  1725.  6435.  1406.  3120.   363.   117.    25.
      7.     3.     1.     0.   512.     1.   494.    69.    44.     6.
      0.     0.     0.     0.    16.    11.     8.     4.    28.     4.
      1.     0.     0.     0.     1.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.]
 [    0. 23917.  5395.  1022.  2016.   554.   380.   107.    56.    24.
      0.     0.     0.   176.   166.   390.  1331.   194.   212.   464.
    436.   399.   593.   244.    67.    55.   453.    36.    56.    31.
     29.    10.     5.     3.     3.     2.     3.     1.     1.     0.
      1.     0.     1.     0.     1.     0.     1.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.]
 [    0.     0.  1695.  6025.  7042.  8837.  8722.  7445.  6918.  7720.
   5037.  4102.  4025.  2706.   538.  1388.   424.   431.   246.   232.
    725.   187.    48.    16.    11.   150.    89.    15.   113.    53.
     68.    11.    24.    27.     8.     3.     0.     1.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.]
 [    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.   541.   234.    58.    44.    27.   962.   886.
     49.   318.    80.   219.     2.   174.    12.    42.    13.     8.
     26.     1.     2.     1.     0.     0.     1.     1.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.]
 [    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.    71.   295.
     79.    77.   292.   242.   362.    35.    23.    33.     3.    15.
      2.     4.     1.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.]
 [    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     5.
      2.     3.     2.     0.     0.    16.     1.     2.     3.     2.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.]]
Test reward:  -1.0020400000000005
LR:  0.001
replay buffer size:  75000
training steps:  49
Average siam loss:  -0.9445930931251496
Average value from last batch of unclaimed novelty:  tensor(-1.1476, dtype=torch.float64)
Average value from last batch of predicted nov:  -0.33084407398401866
Printing a sample from n step novelty returns, should be around 0 on average, going up to 9 max:  [1.16469425 1.48498906 1.80886228 0.94831235 0.41521035 0.39100366
 0.88095298]
then printing the same exp r:  [-0.16841116547584534, 0.12020346522331238, 0.9557532072067261, 0.5830131769180298, 0.04605986550450325, -0.46937018632888794, 0.9273189306259155]
Printing a sample from n step novelty returns, should be around 0 on average, going up to 9 max:  [-0.66727053 -0.63923873 -0.33344063 -0.15552407  0.02062883 -0.57686288
 -1.06343146 -1.2079422  -1.08911483 -1.14399808 -1.1600857  -0.69307242
 -0.4377275  -0.1718492   0.10802269  0.43103709  0.62113668]
then printing the same exp r:  [-0.4453287422657013, -0.4804593622684479, -0.5082620978355408, -0.3578811585903168, 0.37576863169670105, 0.28266459703445435, -0.08500213176012039, -0.40521207451820374, -0.22524738311767578, -0.28891658782958984, -0.22524738311767578, -0.22524738311767578, -0.28891658782958984, -0.28891658782958984, -0.3173289895057678, -0.16741342842578888, 0.6538280844688416]
Checking the total Q and that Qe samples 4.460929088078328 4.676163261060291
Checking the total Q and that Qe samples 1.9212945634649 3.569162444896285
Checking the total Q and that Qe samples -0.262 -5.298739069604874
Checking the total Q and that Qe samples 1.3953000301743372 1.824370282608637
60 : -1.0008933333333336
[[    0.     0.     0.     0.     6.     7.     0.     3.    29.    35.
      4.     2.     0.     3.     0.     0.     0.     0.     1.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.]
 [    0.     0.     0.   126.   747.   117.     6.   364.   295.    18.
     19.     5.     8.     2.     4.     1.     0.   188.    22.     0.
      0.     0.     0.     0.     0.     0.     1.     1.     1.     0.
      1.     2.     3.     1.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.]
 [    0.     0.   133.  1869.  8014.  1766.  3825.   425.   131.    31.
      8.     3.     1.     0.   561.     2.   848.    77.    50.     7.
      0.     0.     0.     0.    16.    12.     8.     4.    28.     4.
      1.     0.     0.     0.     1.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.]
 [    0. 29558.  6221.  1155.  2408.   588.   435.   116.    60.    27.
      0.     0.     0.   186.   178.   460.  1371.   236.   236.   581.
    508.   508.   700.   269.    76.    62.   840.    70.    57.    34.
     29.    10.     5.     3.     3.     2.     3.     1.     1.     0.
      1.     0.     1.     0.     1.     0.     1.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.]
 [    0.     0.  1982.  7000.  8091.  9840.  9863.  8367.  7532.  8531.
   5517.  4554.  4314.  2973.   732.  1694.   462.   519.   290.   272.
   1007.   232.    55.    24.    15.   179.    97.    23.   179.    90.
     69.    12.    31.    27.     8.     3.     0.     1.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.]
 [    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.   623.   264.    60.    52.    29.   981.  1418.
     60.   538.   101.   306.     3.   403.    16.    76.    20.     9.
     27.     1.     2.     1.     0.     1.     1.     2.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.]
 [    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.    71.   784.
     93.    81.   447.   392.   658.    48.    64.    37.     4.    53.
      2.     5.     2.     0.     1.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.]
 [    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     5.
      2.     5.     3.     0.     0.    17.     2.     2.     3.     2.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.]]
Checking the total Q and that Qe samples -0.45128826 -2.714859197233915
Test reward:  -1.0021600000000006
LR:  0.001
replay buffer size:  75000
training steps:  59
Average siam loss:  -0.9375358538236469
Average value from last batch of unclaimed novelty:  tensor(-1.2863, dtype=torch.float64)
Average value from last batch of predicted nov:  -0.3864873843222372
Checking the total Q and that Qe samples -0.2926 -4.841856380601866
Checking the total Q and that Qe samples -0.4936713450070344 -3.7930118593235
Printing a sample from n step novelty returns, should be around 0 on average, going up to 9 max:  [-0.29945878 -0.09164095  0.12548481 -0.03814545 -0.09121364  0.0192287
  0.01932863  0.20057819  0.38915273  0.53147524  0.56024902  0.48736671
  0.27971607  0.24056732  0.14433864  0.19947121  0.46292898  0.4858806
  0.3271006   0.54571415  0.59334849  0.50565133  0.56976444  0.57399261
  0.55334548]
then printing the same exp r:  [-0.26420527696609497, -0.2625754177570343, 0.1313217729330063, 0.010434086434543133, -0.1558695137500763, -0.03971434384584427, -0.03971434384584427, -0.03749988228082657, -0.03971434384584427, -0.03971434384584427, -0.03971434384584427, 0.19438856840133667, 0.19438856840133667, 0.1189260259270668, -0.03749988228082657, -0.17083284258842468, -0.03749988228082657, 0.19438856840133667, 0.025759438052773476, 0.025759438052773476, 0.1189260259270668, -0.03749988228082657, 0.025759438052773476, 0.050857264548540115, 0.5824689269065857]
Checking the total Q and that Qe samples 2.6718761780862303 3.0987440290750765
Checking the total Q and that Qe samples -0.6827585372831984 1.1599414627168017
Checking the total Q and that Qe samples -1.0426377049549136 0.7487625810290354
Printing a sample from n step novelty returns, should be around 0 on average, going up to 9 max:  [-2.35851209 -2.45742085 -2.42642026 -2.31829065 -2.15661727 -2.07974193
 -0.34996841  0.24067855  0.68273824  0.89668202  0.96658143  1.41562985
  1.08461764  0.50087328]
then printing the same exp r:  [-0.08454835414886475, -0.2441595047712326, -0.21709491312503815, -0.21709491312503815, -0.12378774583339691, -0.18543727695941925, -0.21709491312503815, -0.08454835414886475, -0.02270561084151268, -0.02270561084151268, -0.3981757164001465, 0.405519038438797, 0.6408295035362244, 0.5272350311279297]
70 : -1.0
[[    0.     0.     0.     0.     6.     8.     0.     3.    31.    48.
      4.     2.     1.     3.     0.     0.     0.     0.     1.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.]
 [    0.     0.     0.   152.   783.   130.     7.   433.   396.    23.
     21.     8.    10.     5.     5.     1.     0.   188.    23.     0.
      0.     0.     0.     0.     0.     0.     1.     1.     1.     0.
      1.     2.     3.     1.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.]
 [    0.     0.   149.  2096. 10534.  2269.  4309.   472.   147.    37.
      9.     3.     1.   614.   733.     2.   868.    78.    50.     7.
      0.     0.     0.     0.    16.    12.     8.     4.    28.     4.
      1.     0.     0.     0.     1.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.]
 [    0. 35812.  6915.  1255.  2834.   617.   486.   121.    60.    27.
      0.     0.     0.   188.   183.   460.  1372.   236.   240.   589.
    514.   524.   734.   280.    78.    64.  1032.    79.    61.    42.
     29.    10.     5.     3.     3.     2.     3.     1.     1.     0.
      1.     0.     1.     0.     1.     0.     1.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.]
 [    0.     0.  2019.  7123.  8334. 10053. 10007.  8462.  7594.  8567.
   5553.  4590.  4350.  3036.   739.  1740.   466.   560.   294.   286.
   1252.   246.    58.    24.    15.   191.    99.    23.   179.    90.
     69.    12.    31.    27.     8.     3.     0.     1.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.]
 [    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.   635.   276.    60.    52.    29.   981.  1633.
     62.   562.   301.   306.     3.   403.    16.    76.    20.     9.
     27.     1.     2.     1.     0.     1.     1.     2.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.]
 [    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.    71.   828.
     93.    81.   519.   405.   679.    49.    64.    37.     4.    53.
      2.     5.     2.     0.     1.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.]
 [    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     5.
      2.     5.     3.     0.     0.    18.     2.     2.     3.     2.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.]]
Test reward:  -1.0009600000000005
LR:  0.001
replay buffer size:  75000
training steps:  69
Average siam loss:  -0.9260021091904491
Average value from last batch of unclaimed novelty:  tensor(-0.8952, dtype=torch.float64)
Average value from last batch of predicted nov:  -0.28933648204798545
Checking the total Q and that Qe samples 3.5160212032468285 4.371936334765627
Checking the total Q and that Qe samples 3.8020388071040334 4.5089518795193015
Checking the total Q and that Qe samples 4.2130356322526925 4.416035632252693
Checking the total Q and that Qe samples -0.32928774176965586 0.3348843870806311
Checking the total Q and that Qe samples -0.31766611166038516 0.05780803833961483
Printing a sample from n step novelty returns, should be around 0 on average, going up to 9 max:  [-4.37530983 -4.29049056 -4.31606038 -4.33833994 -4.37958203 -4.2659313
 -4.41366832 -4.40964743 -4.30357049 -4.26305045 -4.38582849 -4.34963781
 -4.31433372 -4.41653374 -4.39617185 -4.39258546 -2.12080121 -1.63291955
 -1.32247473 -1.25658244 -0.72719607 -0.20082116  0.00079854 -0.14677366]
then printing the same exp r:  [-0.5724573731422424, -0.5654399394989014, -0.5654399394989014, -0.5724573731422424, -0.601513683795929, -0.4621514678001404, -0.601513683795929, -0.5955222845077515, -0.6275821924209595, -0.4621514678001404, -0.6275821924209595, -0.601513683795929, -0.4621514678001404, -0.5955222845077515, -0.5955222845077515, -0.5955222845077515, -0.5654399394989014, -0.5654399394989014, -0.5724573731422424, -0.5955222845077515, -0.5646483898162842, -0.2121892273426056, 0.1476142257452011, -0.15449859201908112]
80 : -1.0010133333333335
[[    0.     0.     0.     0.     8.    10.     0.     3.    37.    66.
      4.     3.     1.     3.     0.     0.     0.     0.     1.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.]
 [    0.     0.     0.   210.   922.   161.    10.   661.   558.    36.
     36.    11.    24.     8.    19.     1.     0.   253.    28.     0.
      0.     0.     0.     0.     0.     0.     2.     2.     1.     0.
      1.     2.    18.     1.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.]
 [    0.     0.   174.  2386. 11935.  2529.  4424.   669.   173.    46.
     11.     3.     1.   653.   941.    16.   880.    86.    53.     9.
      0.     0.     0.     0.    16.    15.    10.     7.    36.     6.
      1.    18.     1.     0.     1.     0.     0.     0.    10.     1.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.]
 [    0. 43064.  7627.  1391.  3175.   748.   589.   149.    75.    35.
      0.     0.     0.   193.   194.   473.  1404.   257.   278.   737.
    628.   711.  1230.   510.   109.    93.  1164.   124.   130.   243.
     79.   115.     6.     3.     4.    18.    27.     3.     2.     0.
      1.     0.     1.     6.     2.     0.     1.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.]
 [    0.     0.  2259.  7881.  9245. 11131. 10934.  9431.  8449.  9212.
   6225.  5017.  4624.  3490.  1074.  2408.   557.   698.   388.   349.
   1390.   386.   110.    29.    18.   308.   130.    53.   262.   148.
     93.    39.    63.    48.    31.     8.     0.     3.     0.     1.
      0.     1.     0.     1.     0.     1.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.]
 [    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.   657.   284.    64.    53.    29.  1076.  1873.
     86.   604.   338.   319.     4.   423.    25.    94.    26.    12.
     28.     3.     4.     2.     0.     1.     2.     3.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.]
 [    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.    71.  1074.
    376.    91.   578.   692.   876.    69.    73.    43.     9.    55.
      2.     9.     3.     0.     1.     0.     3.     1.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.]
 [    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     5.
      2.     9.     3.     0.     0.    20.     7.     4.     4.     2.
      1.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.]]
Checking the total Q and that Qe samples -0.6343334295960772 -4.59372417836544
Checking the total Q and that Qe samples -0.342511249674243 -4.299256148518045
Test reward:  -1.0016800000000006
LR:  0.001
replay buffer size:  75000
training steps:  79
Average siam loss:  -0.926477383589372
Average value from last batch of unclaimed novelty:  tensor(-1.4500, dtype=torch.float64)
Average value from last batch of predicted nov:  -0.4124236948919869
Checking the total Q and that Qe samples 0.11555618674740975 1.5720620992136347
Checking the total Q and that Qe samples 3.8334799479315835 4.353669034387951
Printing a sample from n step novelty returns, should be around 0 on average, going up to 9 max:  [-3.47518805 -3.5138724  -3.47684072 -3.44387037 -3.43110491 -3.44740725
 -3.41288777 -3.40404434 -3.4220074  -3.48903687 -3.47070582 -1.32118482
 -0.69450022 -0.54446082 -0.271178   -0.08564628  0.05547942 -0.15355544
 -0.2424771  -0.56400736 -0.54546892 -0.55411922 -0.54120402 -0.53907954
 -0.39312069 -0.36637391 -0.21763955 -0.07162739  0.09261994  0.3424606 ]
then printing the same exp r:  [-0.316766619682312, -0.39746150374412537, -0.35721224546432495, -0.33527207374572754, -0.3397713005542755, -0.35721224546432495, -0.3610154390335083, -0.316766619682312, -0.316766619682312, -0.3610154390335083, -0.316766619682312, -0.35721224546432495, -0.33527207374572754, -0.39746150374412537, -0.3397713005542755, -0.27487805485725403, 0.07198777049779892, -0.05656849592924118, 0.185376837849617, -0.17132402956485748, -0.15746687352657318, -0.17132402956485748, -0.1680171936750412, -0.14990247786045074, -0.14952707290649414, -0.1680171936750412, -0.15746687352657318, -0.1680171936750412, -0.24496592581272125, 0.3604848384857178]
90 : -1.0001066666666667
[[    0.     0.     0.     0.    12.    11.     0.     3.    43.    82.
      4.     3.     1.     3.     0.     0.     1.     1.     1.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.]
 [    0.     0.     0.   261.  1044.   214.    11.   806.   733.    45.
     46.    17.    34.    14.    86.    34.     3.   263.    29.     0.
      0.     0.     0.     0.     0.     0.     2.     2.     1.     0.
      1.     2.    18.     1.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.]
 [    0.     0.   186.  2812. 14780.  2942.  5478.   773.   189.    53.
     12.     3.     1.  1145.   974.    26.   975.    87.    53.     9.
      0.     0.     0.     0.    16.    15.    10.     7.    36.     6.
      1.    18.     1.     0.     1.     0.     0.     0.    10.     1.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.]
 [    0. 48694.  8276.  1467.  3605.   785.   652.   157.    77.    35.
      0.     0.     0.   200.   198.   486.  1406.   258.   291.   758.
    638.   759.  1277.   535.   115.    99.  1316.   148.   148.   394.
     93.   118.     6.     3.     4.    18.    27.     3.     2.     0.
      1.     0.     1.     6.     2.     0.     1.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.]
 [    0.     0.  2359.  8110.  9410. 11312. 11080.  9492.  8488.  9265.
   6264.  5056.  4663.  3552.  1097.  2483.   570.   722.   416.   360.
   1595.   416.   114.    31.    19.   335.   137.    55.   263.   151.
     94.    50.   105.    61.    43.     9.     0.     3.     0.     1.
      0.     1.     0.     1.     0.     1.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.]
 [    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.   678.   290.    66.    53.    29.  1076.  1879.
     91.   606.   344.   319.     4.   423.    26.    95.    26.    12.
     28.     3.     4.     2.     0.     6.     2.     3.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.]
 [    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.    71.  1074.
    376.    91.   613.   780.   922.    73.    74.    43.     9.    55.
      2.     9.     3.     0.     1.     0.     3.     1.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.]
 [    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     5.
      2.     9.     3.     0.     0.    21.     8.     5.     4.     2.
      1.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.]]
Printing a sample from n step novelty returns, should be around 0 on average, going up to 9 max:  [2.59336119 2.5933612  2.59336174 2.5933618  2.59336121 2.59336129
 2.59336127 2.59336065 2.59336148 2.59336157 2.59336207 2.59336266
 2.5933636  2.59336291 2.59336302 2.59336254 2.59336198 2.59336103
 2.59336163 2.5933625  2.59336305 2.59336318 2.59336376 2.59336303
 2.59336202 2.59336167 2.59336173 2.59336137 2.59336111 2.59336114
 2.59336174 2.59336263 2.59336295 2.5933639  2.59336495 2.59336488
 2.59336486 2.59336443 2.59336356 2.59336341 2.59336272 2.59336176
 2.59336265 2.59336359 2.59336267 2.59336331 2.59336429 2.59336364
 2.59336277 2.59336376 2.59336309 2.59336215 2.59336244 2.59336331
 2.59336236 2.59336299 2.59336316 4.32949622 1.84101154 1.80209154
 1.53010777 1.53010819 1.5301068  2.085224   1.54945797 1.54945744
 1.54945814 1.54945903 1.54945867 1.54945808 2.15672179 1.56207135
 1.56207152 2.52581608 1.51572306 1.51572258 1.51572199 1.51572125
 1.51572105 1.51572047 1.51571995 1.51571993 1.51572045 2.7450262
 1.73176046 1.73176121 1.7317616  1.73176096 1.73176048 1.73176018
 3.33727812 3.59502469 4.54445543 4.16847568 4.68613538 4.48619096
 4.67576728 4.63529854 4.59157987 4.43388332 4.43387985 4.43387621
 4.43387237 4.4338684  4.43386415 4.43386415 4.43386415 4.43386415
 4.43386415 4.43386415 4.43386415 4.43386415 4.43386415 4.43386415
 4.4338648  4.43386483 4.43386487 4.4338649  4.43386494 4.43386415
 4.43386415 4.43386415 4.43386415 4.29814297 3.52436549 2.70986289
 1.85249172 0.94999575]
then printing the same exp r:  [0.9999983310699463, 0.9999982714653015, 0.9999982714653015, 0.999998927116394, 0.9999982714653015, 0.9999983310699463, 0.999998927116394, 0.9999982714653015, 0.9999982714653015, 0.9999983310699463, 0.9999982714653015, 0.9999982714653015, 0.9999993443489075, 0.9999983310699463, 0.999998927116394, 0.999998927116394, 0.9999993443489075, 0.9999985694885254, 0.9999983310699463, 0.9999983310699463, 0.9999982714653015, 0.9999983310699463, 0.9999994039535522, 0.9999994039535522, 0.999998927116394, 0.9999982714653015, 0.999998927116394, 0.9999985694885254, 0.9999982714653015, 0.9999985694885254, 0.9999982714653015, 0.9999985694885254, 0.9999982714653015, 0.9999982714653015, 0.9999994039535522, 0.9999993443489075, 0.999998927116394, 0.9999993443489075, 0.9999994039535522, 0.9999993443489075, 0.9999993443489075, 0.9999982714653015, 0.9999982714653015, 0.9999993443489075, 0.9999985694885254, 0.9999982714653015, 0.9999993443489075, 0.9999993443489075, 0.9999982714653015, 0.9999993443489075, 0.9999993443489075, 0.9999985694885254, 0.9999983310699463, 0.9999994039535522, 0.9999985694885254, 0.9999982714653015, 0.999998927116394, 0.9999993443489075, 0.9999983310699463, 0.9999993443489075, 0.9999983310699463, 0.9999999403953552, 0.9999927282333374, 1.0, 1.0, 0.999998927116394, 0.9999986290931702, 0.9999999403953552, 1.0, 0.999999463558197, 0.9999997019767761, 0.9999995231628418, 0.9999995231628418, 0.9999993443489075, 1.0, 1.0, 0.9999997019767761, 1.0, 1.0, 0.9999994039535522, 0.9999992847442627, 0.9999987483024597, 0.9999998807907104, 0.9999994039535522, 0.9999987483024597, 0.9999992847442627, 0.9999992847442627, 1.0, 0.9999999403953552, 0.9999995231628418, 0.9999997019767761, 0.9999983310699463, 0.9999995231628418, 0.9999997019767761, 0.9999994039535522, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.9999999403953552, 1.0, 0.9999955296516418, 0.9999955296516418, 0.9999955296516418, 0.9999955296516418, 0.9999955296516418, 0.9999955296516418, 0.9999955296516418, 0.9999955296516418, 0.9999955296516418, 0.9999955296516418, 0.9999955296516418, 0.9999955296516418, 0.9999955296516418, 0.9999955296516418, 0.9999963641166687, 0.9999955296516418, 0.9999955296516418, 0.9999955296516418, 0.9999955296516418, 0.9999955296516418, 0.9999955296516418, 0.9999955296516418, 0.9999955296516418, 0.9999955296516418]
Checking the total Q and that Qe samples -0.21612526337791146 1.390150809176354
Checking the total Q and that Qe samples -0.629472552196647 -1.3348277748685444
Checking the total Q and that Qe samples -0.6529 -2.693422981780093
Test reward:  -1.0015200000000004
LR:  0.001
replay buffer size:  75000
training steps:  89
Average siam loss:  -0.9195485634263605
Average value from last batch of unclaimed novelty:  tensor(-1.6480, dtype=torch.float64)
Average value from last batch of predicted nov:  -0.4500761101990391
Checking the total Q and that Qe samples -0.35361249529445554 -6.417077361689138
Checking the total Q and that Qe samples -0.4234485435022592 -7.244387239823219
Checking the total Q and that Qe samples -0.27918719999999997 -4.073169170869588
Checking the total Q and that Qe samples 2.8538975338612387 3.3635733547037114
Printing a sample from n step novelty returns, should be around 0 on average, going up to 9 max:  [-0.24015133  0.52361256  0.80494456  0.94257146  1.19285134  0.97584905
  0.2624665 ]
then printing the same exp r:  [-0.24253052473068237, -0.38778701424598694, -0.09526139497756958, -0.20067085325717926, 0.2797839343547821, 0.7647430300712585, 0.2762805223464966]
Checking the total Q and that Qe samples 0.1543305507738144 0.9847305507738144
Checking the total Q and that Qe samples -0.52432515 -3.5976114200578624
100 : -1.0002666666666669
[[    0.     0.     0.     0.    14.    11.     0.     3.    44.    90.
      4.     3.     1.     3.     1.     1.     2.     5.     2.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.]
 [    0.     0.     0.   286.  1269.   259.    12.   931.   870.    69.
     64.    41.    52.    38.   346.   241.    26.   383.    58.     0.
      0.     0.     0.     0.     0.     0.     3.     2.     1.     0.
      1.     2.    18.     1.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.]
 [    0.     0.   210.  3130. 16219.  3164.  5578.   868.   199.    61.
     12.     3.     2.  1413.  1088.    40.  1015.   140.    60.    13.
      0.     0.     0.     0.    16.    20.    23.     9.    38.     7.
      1.    19.     3.     1.     1.     0.     0.     0.    11.     2.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.]
 [    0. 56587.  9141.  1526.  3861.   809.   721.   164.    77.    35.
      0.     0.     0.   225.   217.   540.  1462.   314.   328.   851.
    722.   959.  1565.   677.   156.   126.  1456.   205.   213.   509.
    151.   158.    11.     3.     4.    18.    34.     5.     3.     0.
      1.     0.     1.     6.     2.     0.     1.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.]
 [    0.     0.  2880.  9784. 11144. 13025. 12846. 10818.  9420. 10226.
   7098.  5719.  5291.  3955.  1297.  2671.   638.   847.   461.   435.
   1856.   504.   154.    50.    25.   394.   167.    62.   278.   176.
    101.    87.   157.    91.    63.    35.     3.     5.     0.     1.
      0.     1.     0.     1.     0.     1.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.]
 [    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.   741.   327.    80.    60.    30.  1092.  1991.
    117.   652.   414.   414.     6.   434.    29.   100.    29.    14.
     30.     3.     4.     2.     1.    88.    10.     6.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.]
 [    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.    71.  1089.
    445.    98.   792.   837.   969.    87.    87.    44.    11.    55.
     18.    12.     5.     1.     1.     0.    25.     1.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.]
 [    0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     5.
      2.    10.     3.     0.     0.    23.    10.     6.     4.     3.
      1.     0.     1.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.     0.     0.     0.     0.     0.     0.     0.     0.
      0.     0.]]
