{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step(env,action):\n",
    "    \"\"\"real interactions with the environment\"\"\"\n",
    "    real_state = np.random.randn(10,5,5)\n",
    "    reward = np.random.uniform(0,1)\n",
    "    return real_state, reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def representation(Observations):\n",
    "    \"\"\"Turns a set of previous observations into a state embedding representation\"\"\"\n",
    "    return np.mean(Observations,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_P_and_V(state0): \n",
    "    \"\"\"Predicts P and V for this step - used purely for training\"\"\"\n",
    "    P = np.random.randn(5) * state0\n",
    "    V = np.max(state0) * np.random.uniform(0,1)\n",
    "    return P, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dynamics(state0, action):\n",
    "    \"\"\"Takes a state0 and action pair, and predicts the next hidden state representation and immedate reward\"\"\"\n",
    "    state1 = np.random.uniform(0,1) * state0 * np.log(action+2)\n",
    "    reward1 = np.random.randn(1)\n",
    "    return state1, reward1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(t, observations, rewards, future_rewards, true_policies, actions, k):\n",
    "    ### Loss functions useds\n",
    "    def mse(a,b):\n",
    "        return (a-b)**2\n",
    "    def centropy(a,b):\n",
    "        return np.sum(a * np.log(b))\n",
    "    \n",
    "    \"\"\"takes the obs up to time t\"\"\"\n",
    "    \"\"\"At each subsequent time state upTo k...\"\"\"\n",
    "    \"\"\"predict pi and v\"\"\"\n",
    "    \"\"\"Take real action, and predict r\"\"\"\n",
    "    \"\"\"compare these to true pis, true future_rewards and true r\"\"\"\n",
    "    \n",
    "    o = observations[:t] #all observations up to time t\n",
    "    s = representation(o) #first representation of state at time t\n",
    "    loss_p = 0\n",
    "    loss_v = 0\n",
    "    loss_r = 0\n",
    "    for i in range(k):\n",
    "        ##\n",
    "        true_p = true_policies[t+k]\n",
    "        true_fr = future_rewards[t+k]\n",
    "        p, v = predict_P_and_V(s)\n",
    "        loss_p += centropy(p,true_p)\n",
    "        loss_v = mse(true_fr, v)\n",
    "        s, r = dynamics(s, actions[t+k])\n",
    "        loss_r += mse(r, rewards[t+k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self,parent,state):\n",
    "        self.Qsa = 0\n",
    "        self.Nsa = 0\n",
    "        self.Psa = 0\n",
    "        self.Rsa = 0\n",
    "        self.Ssa = 0\n",
    "        self.parent = parent\n",
    "        self.children = []\n",
    "        self.state = state\n",
    "        \n",
    "class MCTS:\n",
    "    def __init__(self,params: dict):\n",
    "        self.c1, self.c2, self.gamma = params\n",
    "    \n",
    "    def one_turn(self,root_node,time_limit=100):\n",
    "        tn = time.time()\n",
    "        self.nodes = []\n",
    "        self.root_node = root_node \n",
    "        while time.time() < tn + time_limit:\n",
    "            mcts_go(root_node)\n",
    "        policy, chosen_action = randomly_sample_action(self.root_node)\n",
    "        return policy, chosen_action\n",
    "        \n",
    "    def mcts_go(self,node):\n",
    "        if len(node.children) == 0:\n",
    "            self.expand(node)\n",
    "        else:\n",
    "            best_ucb_child = self.pick_node_to_expand(node)\n",
    "            mcts_go(best_ucb_child)\n",
    "                \n",
    "    def expand(self,node):\n",
    "        \"\"\"You've reached a terminal node. Backpropogate the rewards and expand the node.\"\"\"\n",
    "        prob_action,V = predict_P_and_V(node.state)\n",
    "        self.back_prop_rewards(node,V)\n",
    "        ## Add a child node for each action of this node.\n",
    "        for edge in actions:\n",
    "            state, _ = dynamics(node.state,actions)\n",
    "            new_node = Node(state, node)\n",
    "            new_node.Psa = prob_action[edge] #set its probability according to the action index from the Ï€ calculated for the whole parent state\n",
    "            self.nodes.append(new_node.copy())\n",
    "        \n",
    "    def pick_node_to_expand(self,node):\n",
    "        return np.argmax([UCB_calc(x) for x in node.children])\n",
    "    \n",
    "    def UCB_calc(self,node):        \n",
    "        Q = node.Qsa\n",
    "        policy_and_novelty_coef = node.Psa * np.sqrt(node.parent.Nsa) / (1+node.Nsa)\n",
    "        muZeroModerator = self.c1 + np.log((node.parent.Nsa + self.c2 + c1+1)/self.c2)\n",
    "        return Q + policy_and_novelty_coef * muZeroModerator\n",
    "    \n",
    "    def back_prop_rewards(self, V):\n",
    "        \"\"\"just send those rewards up the chain\"\"\"\n",
    "    \n",
    "    def randomly_sample_action(self,root_node):\n",
    "        policy = np.array([x.Nsa for x in root_node.children])\n",
    "        return policy, np.random.choice(policy)\n",
    "\n",
    "class Episode:\n",
    "    def __init__(self,params):\n",
    "        self.params = params #c1, c2, gamma, max turns etc.\n",
    "    \n",
    "    def play_episode(self, env):\n",
    "        metrics = {}\n",
    "        for met in ['policy','action','obs','reward']:\n",
    "            metrics[met] = []\n",
    "        obs, _ = env.reset()\n",
    "        metrics['obs'].append(obs)\n",
    "        while True:\n",
    "            state = representation(np.array(metrics['obs'])) #variable length array here, of t x OBS dimension\n",
    "            root_node = Node(parent='null',state=state)\n",
    "            mcts = MCTS(self.params)\n",
    "            policy, action = mcts.one_turn(root_node)\n",
    "            obs, reward, done = step(env, action)\n",
    "            self.store_metrics(policy, action, reward, obs)\n",
    "            if done == True or turn > self.params.turn_limit: \n",
    "                break #params for ending episode\n",
    "        self.calculate_V_from_rewards() #using N step returns or whatever to calculate the returns.\n",
    "        \n",
    "    def store_metrics(self,policy, action, reward,obs):\n",
    "        metrics['obs'].append(obs)\n",
    "        metrics['policy'].append(policy)\n",
    "        metrics['action'].append(action)\n",
    "        metrics['reward'].append(reward)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-0.02382524,  0.12484224, -0.51097075, -0.0836743 , -0.22833026],\n",
       "        [-0.09649986,  0.02579445, -0.20978821, -0.14017272,  0.32218067],\n",
       "        [-0.35151722, -0.12334596, -0.4805248 ,  0.03702563,  0.84166641],\n",
       "        [-1.45884413,  0.18921395, -0.05712818, -0.47003721, -0.20508899],\n",
       "        [-0.29056937,  0.20483202, -0.36071345, -0.10936777,  0.91639969],\n",
       "        [ 0.05280136,  0.18237211, -0.40537285, -0.11181813, -0.05391194],\n",
       "        [ 0.66377449, -0.0242699 ,  0.6102202 , -0.0192993 ,  0.70139135],\n",
       "        [-0.31561705,  0.19632571, -0.60327542, -0.04454043, -0.76430236],\n",
       "        [ 0.21008685,  0.12271352,  0.49260207, -0.27185586,  0.5766538 ],\n",
       "        [ 0.11965702,  0.12359691, -0.03796755,  0.0525345 ,  0.5754395 ]]),\n",
       " 0.7276996333402753)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env= 'k'\n",
    "Observation0,real_reward = receive_state_and_reward(env,action=0)\n",
    "state0 = representation(Observation0)\n",
    "predict_P_and_V(state0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
